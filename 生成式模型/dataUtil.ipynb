{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n把数据转换为  原名+train   原名title  存在另一个叫 train的目录下\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "把raw数据转换为  content+原名   title+原名  存目录下\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入原数据  输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strQ2B(ustring):\n",
    "    \"\"\"全角转半角\"\"\"\n",
    "    rstring = \"\"\n",
    "    for uchar in ustring:\n",
    "        inside_code=ord(uchar)\n",
    "        if inside_code == 12288:                              #全角空格直接转换            \n",
    "            inside_code = 32 \n",
    "        elif (inside_code >= 65281 and inside_code <= 65374): #全角字符（除空格）根据关系转化\n",
    "            inside_code -= 65248\n",
    "\n",
    "        rstring += chr(inside_code)\n",
    "    return rstring\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(text,vocab):\n",
    "    text=strQ2B(text)\n",
    "    text=re.sub(r'\\ue40c','',text)\n",
    "    text=re.sub('[:「」￥…，,(【嘻嘻】)【哈哈】;\"”“+/—!. _ - % \\[\\]*◎《》、。]', '', text)  # 去特殊字符\n",
    "    text=re.sub('(www\\.(.*?)\\.com)|(http://(.*?)\\.com)','',text.lower()) #去URL\n",
    "    text=re.sub('[a-zA-Z]+',' EN ',text) # 去英文\n",
    "    text=re.sub('([\\d]*年*[\\d]*月*[\\d]+日+)|([\\d]+年+)|([\\d]*年*[\\d]+月+)',' DATE ',text)#去日期\n",
    "    text=re.sub('[\\d]+',' NUMBER ',text) #去数字\n",
    "    seg_list = jieba.cut(text, cut_all=False)\n",
    "    text=' '.join(seg_list)\n",
    "    for word in text.split(' '):\n",
    "        vocab[word]=vocab.get(word,0)+1\n",
    "    #结巴分词\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raw_form(path,title_path,content_path,vocab):\n",
    "    contents=[]\n",
    "    titles=[]\n",
    "    with open(path,'r',encoding='ansi')as f:\n",
    "        text=''.join(f.readlines())\n",
    "        p=re.compile('<contenttitle>(.*)</contenttitle>\\n<content>(.*)</content>')\n",
    "        for temp in p.finditer(text):\n",
    "            title=(temp.group(1))\n",
    "            content=(temp.group(2))\n",
    "            if title.strip()!='' and content.strip()!='':\n",
    "                s1=clean(title,vocab)\n",
    "                if len(s1)>60 :\n",
    "                    s1=s1[:60]\n",
    "                s2=clean(content,vocab)\n",
    "                if len(s2)>240 :\n",
    "                    s2=s2[:240]\n",
    "                titles.append(s1)\n",
    "                contents.append(s2)\n",
    "    with open(title_path,'w',encoding='utf-8') as wf:\n",
    "        for temp in titles:\n",
    "            wf.write(temp+'\\n')\n",
    "    with open(content_path,'w',encoding='utf-8') as wf:\n",
    "        for temp in contents:\n",
    "            wf.write(temp+'\\n')\n",
    "    return contents,titles\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['news.sohunews.240806.txt']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path='./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_path=\"./data_test\"\n",
    "form_path='./train_test'\n",
    "vocab_path='./vocab/vocab.txt'\n",
    "max_vocabulary_size=50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_raw_fom(raw_path,form_path,vocab_path,max_vocabulary_size):\n",
    "    vocab={}\n",
    "    special_words = ['<PAD>', '<UNK>', '<GO>',  '<EOS>']\n",
    "    file_list=os.listdir(raw_path)\n",
    "    print(file_list)\n",
    "    for item in file_list:\n",
    "        _,_=raw_form(os.path.join(raw_path,item),os.path.join(form_path,'title_'+item),os.path.join(form_path,'content_'+item),vocab)\n",
    "    \n",
    "    vocab_list=special_words+sorted(vocab, key=vocab.get, reverse=True)\n",
    "    if len(vocab_list)>max_vocabulary_size:\n",
    "        vocab_list=vocab_list[:max_vocabulary_size]\n",
    "    with open(vocab_path,'w',encoding='utf-8')as f:\n",
    "        for w in vocab_list:\n",
    "            f.write(w+'\\n')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": ['删除的内容']
    }
   ],
   "source": [
    "all_raw_fom(raw_path,form_path,vocab_path,max_vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入字典  生成字典  输出tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fom_tokenize(form_path,tokenize_path,vocab_path):\n",
    "\n",
    "    vocab_list=[]\n",
    "    with open(vocab_path,'r',encoding='utf-8')as f:\n",
    "        for item in f.readlines():\n",
    "            vocab_list.append(item.strip())\n",
    "    int_to_vocab = {idx: word for idx, word in enumerate(vocab_list)}\n",
    "    vocab_to_int = {word: idx for idx, word in int_to_vocab.items()}\n",
    "    file_list=os.listdir(form_path)\n",
    "    print(file_list)\n",
    "    for item in file_list:\n",
    "        with open(os.path.join(form_path,item),'r',encoding='utf-8')as f:\n",
    "#             tokenize=[]\n",
    "#             for sentence in f.readlines():\n",
    "#                 sentence_list=sentence.strip().split(' ')\n",
    "#                 temp=[]\n",
    "#                 for word in sentence_list:\n",
    "#                     temp.append()\n",
    "            if re.compile('title').search(item):\n",
    "                tokenize=[[vocab_to_int.get(word,vocab_to_int['<UNK>']) for word in sentence.strip().split(' ')]+[vocab_to_int['<EOS>']]for sentence in f.readlines()]\n",
    "            else:\n",
    "                tokenize=[[vocab_to_int.get(word,vocab_to_int['<UNK>']) for word in sentence.strip().split(' ')]for sentence in f.readlines()]\n",
    "            with open(os.path.join(tokenize_path,item),'w',encoding='utf-8')as wf:\n",
    "                for line in tokenize:\n",
    "                    s=' '.join([str(w) for w in line])\n",
    "                    wf.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "form_path='./train_test'\n",
    "tokenize_path='./tokenize'\n",
    "vocab_path='./vocab/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['content_news.sohunews.010806.txt', 'content_news.sohunews.020806.txt', 'title_news.sohunews.010806.txt', 'title_news.sohunews.020806.txt']\n"
     ]
    }
   ],
   "source": [
    "fom_tokenize(form_path,tokenize_path,vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
